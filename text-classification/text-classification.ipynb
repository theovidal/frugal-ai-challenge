{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Text classification for disinformation\n",
    "\n",
    "[Dataset](https://huggingface.co/datasets/QuotaClimat/frugalaichallenge-text-train)\n",
    "\n",
    "Goal: multi-classification task with 8 labels for types of climate disinformation statements"
   ],
   "id": "53c7bafd439caae3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import libraries and select device",
   "id": "dafc9b9281ba1082"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import plotly.express as px\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "from torchinfo import summary\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint"
   ],
   "id": "3569cf2aa516f5b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device {device}')\n",
    "%env PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
   ],
   "id": "1158e875fbcdfcda",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataset pipeline",
   "id": "a583ad44474ce3e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Loading and Train/Validation/Test split",
   "id": "ec8acba263e90112"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "batch_size = 64\n",
    "sequence_length = 512\n",
    "num_workers = 4\n",
    "\n",
    "train_validation_split = 0.2"
   ],
   "id": "92208effd0d0fff5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_dataset = load_dataset(\"QuotaClimat/frugalaichallenge-text-train\", split=\"train\").with_format(type='torch')\n",
    "\n",
    "unique_labels = list(set(train_dataset[\"label\"]))  # Extract unique label names\n",
    "unique_labels.sort()\n",
    "label_to_id = {label: i for i, label in enumerate(unique_labels)}  # Assign unique IDs\n",
    "\n",
    "indices = torch.randperm(len(train_dataset)) # Generate a permutation between 0 and (train_size - 1) ; train_loader will get images in the order of the indices (so producing a random sampling)\n",
    "train_validation_index = int(len(train_dataset) * train_validation_split) # Never forget \"int\" otherwise it's a float!!\n",
    "train_indices = indices[train_validation_index:]\n",
    "val_indices = indices[:train_validation_index]\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torch.utils.data.Subset(train_dataset, train_indices),\n",
    "  num_workers=num_workers,\n",
    "  batch_size=batch_size,\n",
    "  # pin_memory=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "  torch.utils.data.Subset(train_dataset, val_indices),\n",
    "  num_workers=num_workers,\n",
    "  batch_size=batch_size,\n",
    "  # pin_memory=True\n",
    ")"
   ],
   "id": "da64d265cee49753",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_dataset = load_dataset(\"QuotaClimat/frugalaichallenge-text-train\", split=\"test\").with_format(type='torch')\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  test_dataset,\n",
    "  batch_size=batch_size,\n",
    "  num_workers=num_workers,\n",
    "  # pin_memory=True\n",
    ")"
   ],
   "id": "58051da4386e746a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tokenization (only for DistilBERT)",
   "id": "4813f6aa736ae3bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\", do_lower_case=True)\n",
    "\n",
    "def collate_fn(batch):\n",
    "  batch_dict = default_collate(batch)\n",
    "  labels = F.one_hot(batch_dict[\"label\"], num_classes=len(label_to_id)).float() # Floating point for comparison between probabilities\n",
    "\n",
    "  return (batch_dict['input_ids'], batch_dict['attention_mask']), labels\n",
    "\n",
    "def tokenization(example):\n",
    "    tokenized = tokenizer(\n",
    "      example[\"quote\"],\n",
    "      truncation=True,\n",
    "      padding='max_length',\n",
    "      max_length=sequence_length\n",
    "    )\n",
    "    tokenized['label'] = label_to_id[example['label']]\n",
    "    return tokenized\n",
    "\n",
    "train_dataset = train_dataset.map(tokenization, remove_columns=[\"quote\"])\n",
    "test_dataset = test_dataset.map(tokenization, remove_columns=[\"quote\"])"
   ],
   "id": "79279217ccb6fad3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Skeleton",
   "id": "e59ec2c86f3161da"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Lightning Module",
   "id": "969df2fd129ed09e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T14:29:50.523731Z",
     "start_time": "2025-01-31T14:29:50.512995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LitModule(L.LightningModule):\n",
    "  def __init__(self, model, lr=0.001, top_k=1):\n",
    "    super().__init__()\n",
    "    self.model = model\n",
    "    self.loss = nn.CrossEntropyLoss() # We deal with a multi-classification : an item belongs to a unique class between Nclasses\n",
    "    self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes, top_k=top_k) \n",
    "    self.lr = lr\n",
    "      \n",
    "  def training_step(self, batch, batch_idx):\n",
    "    x, y_true = batch\n",
    "    y_pred = self.model(x)\n",
    "    \n",
    "    loss = self.loss(y_pred, y_true)\n",
    "    accuracy = self.accuracy(y_pred.argmax(dim=1), y_true)\n",
    "    \n",
    "    self.log_dict({\n",
    "      \"train_accuracy\": accuracy,\n",
    "      'train_loss': loss\n",
    "    }, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "    return loss # All the backward stuff is handled with Lightning\n",
    "    \n",
    "  def validation_step(self, batch, batch_idx):\n",
    "    x, y_true = batch\n",
    "    y_pred = self.model(x)\n",
    "\n",
    "    loss = self.loss(y_pred, y_true)\n",
    "    accuracy = self.accuracy(y_pred.argmax(dim=1), y_true)\n",
    "\n",
    "    self.log_dict({\n",
    "      \"val_accuracy\": accuracy,\n",
    "      'val_loss': loss\n",
    "    }, on_step=False, on_epoch=True, prog_bar=True)\n",
    "  \n",
    "  def test_step(self, batch, batch_idx):\n",
    "    x, y_true = batch\n",
    "    y_pred = self.model(x)\n",
    "    \n",
    "    loss = self.loss(y_pred, y_true)\n",
    "    accuracy = self.accuracy(y_pred.argmax(dim=1), y_true)    \n",
    "    self.log_dict({\n",
    "      \"test_accuracy\": accuracy,\n",
    "      'test_loss': loss\n",
    "    }, on_epoch=True, prog_bar=True)\n",
    "    \n",
    "  def configure_optimizers(self):\n",
    "    optimizer = torch.optim.RMSprop(self.parameters(), lr=self.lr) # Difference here: parameters are stored at LightningModule-level, not model level\n",
    "    return optimizer"
   ],
   "id": "a6346472fddfd36f",
   "outputs": [],
   "execution_count": 139
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataset exploration",
   "id": "4623d26851f8acbd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Class repartition",
   "id": "a92bd4adea250581"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_classes = len(unique_labels)\n",
    "print(f'Number of classes: {num_classes}')"
   ],
   "id": "dacb859f5c0d7344",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "unique, counts = np.unique(train_dataset['label'], return_counts=True)\n",
    "\n",
    "px.pie(names=unique, values=counts)"
   ],
   "id": "576f009c5a50ce52",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Models",
   "id": "4086a263334b6feb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## DistilBERT",
   "id": "b2ca69d549a71bdf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class DistilBERTClassifier(nn.Module):\n",
    "  def __init__(self, backbone):\n",
    "    super().__init__()\n",
    "    self.backbone = backbone\n",
    "    self.dropout = nn.Dropout(0.5)\n",
    "    self.linear = nn.Linear(768, num_classes)\n",
    "      \n",
    "  def forward(self, x):\n",
    "    input_ids, attention_mask = x\n",
    "    \n",
    "    x = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    hidden_state = x[0]\n",
    "    pooler = hidden_state[:, 0]\n",
    "    x = self.dropout(pooler)\n",
    "    return F.softmax(self.linear(x), dim=1)"
   ],
   "id": "1afa1570c846be66",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import DistilBertModel\n",
    "\n",
    "distilbert_model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "distilbert_module = DistilBERTClassifier(distilbert_model)\n",
    "distilbert = LitModule(distilbert_module)"
   ],
   "id": "efdfb2e57746ff4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "trainer = L.Trainer(max_epochs=10)\n",
    "trainer.fit(model=distilbert, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ],
   "id": "8037f7cfe928ba84",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Testing the model",
   "id": "edb077dee03c2a3f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "trainer.test(model=distilbert, dataloaders=test_loader)",
   "id": "1b46c962824a0420",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## GloVe\n",
    "\n",
    "[Homepage](https://nlp.stanford.edu/projects/glove/)"
   ],
   "id": "3e6d63a55dc51c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_params = '6B'\n",
    "embedding_dim = 100 # 50, 100, 200 or 300\n",
    "max_sequence_length = 512"
   ],
   "id": "170e692747d6e90",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Loading pre-trained embeddings and create vocabulary",
   "id": "e0e30018653b25d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!wget https://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove.6B.zip\n",
    "!python -m spacy download en_core_web_sm"
   ],
   "id": "40be8d7e90586b19",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "embeddings = {}\n",
    "glove_path = f'glove.{num_params}.{embedding_dim}d.txt'\n",
    "with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "  for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vector = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings[word] = vector"
   ],
   "id": "173fde8e60598a0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "vocabulary = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "\n",
    "def extract_from_example(example):\n",
    "  for token in nlp(example['quote']):\n",
    "    if token.text.lower() not in vocabulary:\n",
    "      index = len(vocabulary)\n",
    "      vocabulary[token.text.lower()] = index\n",
    "\n",
    "train_dataset.map(extract_from_example)\n",
    "test_dataset.map(extract_from_example)\n",
    "\n",
    "vocabulary_size = len(vocabulary)"
   ],
   "id": "93b325b10d175fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "embedding_matrix = np.zeros((vocabulary_size, embedding_dim))\n",
    "\n",
    "for word, i in vocabulary.items():\n",
    "  if word in embeddings.keys():\n",
    "    embedding_matrix[i] = embeddings[word]\n",
    "  else:\n",
    "    embedding_matrix[i] = np.random.normal(scale=0.6, size=embedding_dim)"
   ],
   "id": "c2b4d53f97cbb398",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "68555d9b407993e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Adapting the dataset pipeline",
   "id": "bb73ba497ec458ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "label_to_id",
   "id": "a4226beb2b577ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T14:25:07.203019Z",
     "start_time": "2025-01-31T14:25:07.192376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "vectorized_map = np.vectorize(label_to_id.get)\n",
    "\n",
    "def collate_fn(batch):\n",
    "\n",
    "  # Tokenize all quotes from the batch\n",
    "  batch_dict = { key: [item[key] for item in batch] for key in batch[0]}\n",
    "  labels = np.array(batch_dict[\"label\"])\n",
    "  labels_encoded = torch.LongTensor(vectorized_map(labels)) # Floating point needed for loss calculation\n",
    "  \n",
    "  # The last batch often is shorter than batch_size, so we have to dynamically set the size with len(batch)\n",
    "  token_sequences = torch.zeros((len(batch), max_sequence_length), dtype=torch.long)\n",
    "  \n",
    "  for i, text in enumerate(batch_dict['quote']):\n",
    "    tokens = [vocabulary.get(token.text.lower(), 1) for token in nlp(text)] # Use the \"get\" method to have a fallback if the word is unknown\n",
    "    if len(tokens) < max_sequence_length:\n",
    "      tokens += [0] * (max_sequence_length - len(tokens))\n",
    "    else:\n",
    "      tokens = tokens[:max_sequence_length]\n",
    "    token_sequences[i] = torch.LongTensor(tokens)\n",
    "    \n",
    "  return token_sequences, labels_encoded\n",
    "\n",
    "train_loader.collate_fn = collate_fn\n",
    "val_loader.collate_fn = collate_fn\n",
    "test_loader.collate_fn = collate_fn"
   ],
   "id": "fe902cfa3e82945c",
   "outputs": [],
   "execution_count": 135
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Module training",
   "id": "ef0d6f225769cad5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T14:44:58.096172Z",
     "start_time": "2025-01-31T14:44:58.085879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GloveModule(nn.Module):\n",
    "  def __init__(self, embeddings, hidden_size=128, num_layers=1, freeze=True):\n",
    "    super().__init__()\n",
    "    self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embeddings), freeze=freeze, padding_idx=0)\n",
    "    self.gru = nn.GRU(embedding_dim, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "    self.dropout = nn.Dropout(0.5)\n",
    "    self.linear = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    x = self.embedding(x)\n",
    "    _, h_n = self.gru(x)\n",
    "    x = self.dropout(F.relu(h_n[-1])) # We take the last state, so the state of the last GRU cell\n",
    "    return self.linear(x)"
   ],
   "id": "44ab466f7ec7a3ba",
   "outputs": [],
   "execution_count": 151
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-31T14:47:07.736351Z",
     "start_time": "2025-01-31T14:44:58.796514Z"
    }
   },
   "cell_type": "code",
   "source": [
    "glove_module = GloveModule(embedding_matrix, num_layers=8)\n",
    "glove = LitModule(glove_module)\n",
    "\n",
    "trainer = L.Trainer(max_epochs=10)\n",
    "trainer.fit(model=glove, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ],
   "id": "c525d650ec994356",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type               | Params | Mode \n",
      "--------------------------------------------------------\n",
      "0 | model    | GloveModule        | 2.4 M  | train\n",
      "1 | loss     | CrossEntropyLoss   | 0      | train\n",
      "2 | accuracy | MulticlassAccuracy | 0      | train\n",
      "--------------------------------------------------------\n",
      "782 K     Trainable params\n",
      "1.6 M     Non-trainable params\n",
      "2.4 M     Total params\n",
      "9.695     Total estimated model params size (MB)\n",
      "7         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "db05793f62d3422eb1e6f30a24c4ee22"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c7a12e56527b4adaab6e8043acadd836"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d319132440884df8906fbab4e49e6413"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fca9293e2fb849beb971ce5563985376"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:47\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[0;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39mtrainer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m---> 47\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtrainer_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _TunerExitException:\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:575\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    569\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39m_select_ckpt_path(\n\u001B[1;32m    570\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn,\n\u001B[1;32m    571\u001B[0m     ckpt_path,\n\u001B[1;32m    572\u001B[0m     model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    573\u001B[0m     model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    574\u001B[0m )\n\u001B[0;32m--> 575\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mckpt_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    577\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:982\u001B[0m, in \u001B[0;36mTrainer._run\u001B[0;34m(self, model, ckpt_path)\u001B[0m\n\u001B[1;32m    979\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m    980\u001B[0m \u001B[38;5;66;03m# RUN THE TRAINER\u001B[39;00m\n\u001B[1;32m    981\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m--> 982\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_stage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    984\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m    985\u001B[0m \u001B[38;5;66;03m# POST-Training CLEAN UP\u001B[39;00m\n\u001B[1;32m    986\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:1026\u001B[0m, in \u001B[0;36mTrainer._run_stage\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1025\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mset_detect_anomaly(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_detect_anomaly):\n\u001B[0;32m-> 1026\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1027\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:216\u001B[0m, in \u001B[0;36m_FitLoop.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_start()\n\u001B[0;32m--> 216\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    217\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end()\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:455\u001B[0m, in \u001B[0;36m_FitLoop.advance\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    454\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_data_fetcher \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 455\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mepoch_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data_fetcher\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py:150\u001B[0m, in \u001B[0;36m_TrainingEpochLoop.run\u001B[0;34m(self, data_fetcher)\u001B[0m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 150\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madvance\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_fetcher\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    151\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mon_advance_end(data_fetcher)\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py:282\u001B[0m, in \u001B[0;36m_TrainingEpochLoop.advance\u001B[0;34m(self, data_fetcher)\u001B[0m\n\u001B[1;32m    281\u001B[0m dataloader_iter \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 282\u001B[0m batch, _, __ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdata_fetcher\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    283\u001B[0m \u001B[38;5;66;03m# TODO: we should instead use the batch_idx returned by the fetcher, however, that will require saving the\u001B[39;00m\n\u001B[1;32m    284\u001B[0m \u001B[38;5;66;03m# fetcher state so that the batch_idx is correct after restarting\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/lightning/pytorch/loops/fetchers.py:134\u001B[0m, in \u001B[0;36m_PrefetchDataFetcher.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    132\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdone:\n\u001B[1;32m    133\u001B[0m     \u001B[38;5;66;03m# this will run only when no pre-fetching was done.\u001B[39;00m\n\u001B[0;32m--> 134\u001B[0m     batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__next__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    135\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    136\u001B[0m     \u001B[38;5;66;03m# the iterator is empty\u001B[39;00m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/lightning/pytorch/loops/fetchers.py:61\u001B[0m, in \u001B[0;36m_DataFetcher.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     60\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 61\u001B[0m     batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     62\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/lightning/pytorch/utilities/combined_loader.py:341\u001B[0m, in \u001B[0;36mCombinedLoader.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    340\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterator \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 341\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_iterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    342\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterator, _Sequential):\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/lightning/pytorch/utilities/combined_loader.py:78\u001B[0m, in \u001B[0;36m_MaxSizeCycle.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     77\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 78\u001B[0m     out[i] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miterators\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     79\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    700\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 701\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    702\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1448\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1447\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_shutdown \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m-> 1448\u001B[0m idx, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1449\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tasks_outstanding \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1412\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._get_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1411\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m-> 1412\u001B[0m     success, data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_try_get_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1413\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m success:\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1243\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m   1242\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1243\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data_queue\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1244\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[38;5;28;01mTrue\u001B[39;00m, data)\n",
      "File \u001B[0;32m/usr/lib/python3.10/multiprocessing/queues.py:122\u001B[0m, in \u001B[0;36mQueue.get\u001B[0;34m(self, block, timeout)\u001B[0m\n\u001B[1;32m    121\u001B[0m \u001B[38;5;66;03m# unserialize the data after having released the lock\u001B[39;00m\n\u001B[0;32m--> 122\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_ForkingPickler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloads\u001B[49m\u001B[43m(\u001B[49m\u001B[43mres\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/torch/multiprocessing/reductions.py:541\u001B[0m, in \u001B[0;36mrebuild_storage_fd\u001B[0;34m(cls, df, size)\u001B[0m\n\u001B[1;32m    540\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrebuild_storage_fd\u001B[39m(\u001B[38;5;28mcls\u001B[39m, df, size):\n\u001B[0;32m--> 541\u001B[0m     fd \u001B[38;5;241m=\u001B[39m \u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdetach\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    542\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m/usr/lib/python3.10/multiprocessing/resource_sharer.py:58\u001B[0m, in \u001B[0;36mDupFd.detach\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _resource_sharer\u001B[38;5;241m.\u001B[39mget_connection(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_id) \u001B[38;5;28;01mas\u001B[39;00m conn:\n\u001B[0;32m---> 58\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mreduction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv_handle\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconn\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/lib/python3.10/multiprocessing/reduction.py:189\u001B[0m, in \u001B[0;36mrecv_handle\u001B[0;34m(conn)\u001B[0m\n\u001B[1;32m    188\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m socket\u001B[38;5;241m.\u001B[39mfromfd(conn\u001B[38;5;241m.\u001B[39mfileno(), socket\u001B[38;5;241m.\u001B[39mAF_UNIX, socket\u001B[38;5;241m.\u001B[39mSOCK_STREAM) \u001B[38;5;28;01mas\u001B[39;00m s:\n\u001B[0;32m--> 189\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mrecvfds\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m/usr/lib/python3.10/multiprocessing/reduction.py:157\u001B[0m, in \u001B[0;36mrecvfds\u001B[0;34m(sock, size)\u001B[0m\n\u001B[1;32m    156\u001B[0m bytes_size \u001B[38;5;241m=\u001B[39m a\u001B[38;5;241m.\u001B[39mitemsize \u001B[38;5;241m*\u001B[39m size\n\u001B[0;32m--> 157\u001B[0m msg, ancdata, flags, addr \u001B[38;5;241m=\u001B[39m \u001B[43msock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecvmsg\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msocket\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCMSG_SPACE\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbytes_size\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    158\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m msg \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m ancdata:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[152], line 5\u001B[0m\n\u001B[1;32m      2\u001B[0m glove \u001B[38;5;241m=\u001B[39m LitModule(glove_module)\n\u001B[1;32m      4\u001B[0m trainer \u001B[38;5;241m=\u001B[39m L\u001B[38;5;241m.\u001B[39mTrainer(max_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m)\n\u001B[0;32m----> 5\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mglove\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_loader\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:539\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    537\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m=\u001B[39m TrainerStatus\u001B[38;5;241m.\u001B[39mRUNNING\n\u001B[1;32m    538\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m--> 539\u001B[0m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    540\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\n\u001B[1;32m    541\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:64\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[0;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m     62\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(launcher, _SubprocessScriptLauncher):\n\u001B[1;32m     63\u001B[0m         launcher\u001B[38;5;241m.\u001B[39mkill(_get_sigkill_signal())\n\u001B[0;32m---> 64\u001B[0m     \u001B[43mexit\u001B[49m(\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exception:\n\u001B[1;32m     67\u001B[0m     _interrupt(trainer, exception)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'exit' is not defined"
     ]
    }
   ],
   "execution_count": 152
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "trainer.test(glove, dataloaders=test_loader)",
   "id": "6b77c15a7f82fd6f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "torch.argmax(glove_module(torch.randint(0, vocabulary_size, (1, 512))), dim=1)",
   "id": "b56e51c26f392717",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
